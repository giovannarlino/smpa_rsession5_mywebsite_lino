[
  {
    "objectID": "02_virginia_election_project_youranalysis.html",
    "href": "02_virginia_election_project_youranalysis.html",
    "title": "Virginia Election Project",
    "section": "",
    "text": "Comparing Virgnia Gov vs. Prez\n\nhead(joined_vacomparison)\n\n# A tibble: 6 × 9\n  locality         biden trump biden_pct trump…¹ young…² mcaul…³ pct_y…⁴ pct_m…⁵\n  <chr>            <dbl> <dbl>     <dbl>   <dbl>   <int>   <int>   <dbl>   <dbl>\n1 ACCOMACK COUNTY   7578  9172      44.7    54.1    7878    4948    61.1    38.4\n2 ALBEMARLE COUNTY 42466 20804      65.7    32.2   19141   31919    37.2    62.0\n3 ALEXANDRIA CITY  66240 14544      80.3    17.6   14013   43866    24.0    75.2\n4 ALLEGHANY COUNTY  2243  5859      27.3    71.4    4530    1518    74.5    25.0\n5 AMELIA COUNTY     2411  5390      30.6    68.3    4720    1617    74.2    25.4\n6 AMHERST COUNTY    5672 11041      33.4    64.9    9731    3897    71      28.4\n# … with abbreviated variable names ¹​trump_pct, ²​youngkin, ³​mcauliffe,\n#   ⁴​pct_youngkin, ⁵​pct_mcauliffe\n\n\n\n#what is the average democratic percentage for both presidential and government election in Accomack County? \n\njoined_vacomparison %>% \n  filter(locality == \"ACCOMACK COUNTY\" ) %>% \n  group_by(biden_pct, pct_mcauliffe) %>% \n  summarise(average = mean(biden_pct)) %>% \n  arrange(desc(average))\n\n`summarise()` has grouped output by 'biden_pct'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 1 × 3\n# Groups:   biden_pct [1]\n  biden_pct pct_mcauliffe average\n      <dbl>         <dbl>   <dbl>\n1      44.7          38.4    44.7\n\n\n\n#create a map of the republican vote in all of the counties for both president and governor \n\n#analyze the counties with the biggest democratic victory \n\n#analyze the counties with the biggest republican victory \n\n#was there a county in which Biden won for which the democratic governor didn't?"
  },
  {
    "objectID": "walkthrough_R_concept.html",
    "href": "walkthrough_R_concept.html",
    "title": "Walkthrough",
    "section": "",
    "text": "Here, I am going to walk you through the steps of loading your packages and dataframe, and using commands such as filter, group_by, mutate and summarise. Then, I will also walk you through the concept of functions and how to create them using certain data sets.\n\nFilter, Group_by, Mutate & Summarise\nFirst, I am loading the data and running the code below to create a dataframe called ‘impeach’. This dataframe contains a row for every House Democrat and whether the member publicly called for impeachment in the case of the first Trump impeachment. There are also other data included related to each politician’s district.\nNow, I am running the code to load the dataframe.\n\nimpeach <- readRDS(\"impeach.rds\")\n\nI will take my dataset, impeach, and count the amount of members that were in favor of impeachment vs. not in favor. To do that, I first group_by for the members in favor, and then I use the summarise(n()) function to count the amount of members for each category (YES or NO).\n\nimpeach %>% \n  group_by(for_impeachment) %>% \n  summarise(n())\n\n# A tibble: 2 × 2\n  for_impeachment `n()`\n  <chr>           <int>\n1 NO                 26\n2 YES               209\n\n# In this case, 26 members voted NO vs. 209 voted YES for impeachment \n\nNow, I can also include to group not only by the amount of members in favor of Trump’s impeachment, but also if the district is above of below the national average for the percentage of people that graduated college. To do that, I follow the same steps as the previous code, but also I add the pct_bachelors_compared_to_national in the group_by function to count how many districts are above or below national average.\nBy grouping both those who are in favor and against impeachment and whether the district is above or below the national average the percentage of college graduates, I was able to summarise the count correlating both factors as showed in the table below\n\nimpeach %>% \n  group_by(for_impeachment, pct_bachelors_compared_to_national) %>% \n  summarise(n())\n\n`summarise()` has grouped output by 'for_impeachment'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 4 × 3\n# Groups:   for_impeachment [2]\n  for_impeachment pct_bachelors_compared_to_national `n()`\n  <chr>           <chr>                              <int>\n1 NO              ABOVE                                  7\n2 NO              BELOW                                 19\n3 YES             ABOVE                                128\n4 YES             BELOW                                 81\n\n\nIn addition, I can also calculate the mean of something within a data frame. For that, I can take the dataset, group by only the members in favor of impeachment and find the average of college grads that were in favor of impeachment. To do that I first put the data set, impeach, then I group_by the for_impeachment row, and use the summarise function. Within the summarise function, I assigned the name average to become the mean of the pct_bachelors row (which showcases the percentage of college grads).\n\nimpeach %>% \n  group_by(for_impeachment) %>% \n  summarise(average = mean(pct_bachelors)) \n\n# A tibble: 2 × 2\n  for_impeachment average\n  <chr>             <dbl>\n1 NO                 27.7\n2 YES                33.7\n\n# Approx. the average 33.7% of college grads are in favor of impeachment and 27.65% are not \n\nAnother good tool I can use is filtering out only the members who are in favor of the impeachment. Then, after filtering it, I can check how many won their 2018 election by less than 5 percentage points vs. more - which is found in the row margin_flag_2018.\nFor that, I first filter by writing the for_impeachment row and assigning it (==) to only equal YES results (those who are in favor of the impeachment). Then, I group_by the margin_flag_2018, which is where I can find the second part of my data. After, I use the function summarise (n()) to find the count of members who were in favor of the impeachment and have won their 2018 election by 5 percentage points or less vs. more.\n\nimpeach %>% \n  filter (for_impeachment == \"YES\") %>% \n  group_by(margin_flag_2018) %>% \n  summarise (n())\n\n# A tibble: 2 × 2\n  margin_flag_2018   `n()`\n  <chr>              <int>\n1 5_points_or_less      17\n2 more_than_5_points   192\n\n# 17 people who were in favor of impeachment won their 2018 election by 5 points or less \n\nFurthermore, I can create a new column by usin the function called mutate(). In this case, I am creating two new columns, one that will extract the year portion of the column date_announced and another which will extract the month of the same column. I have also added the filter() function to only select those in favor of the impeachment. I am doing this to find the year and month that people in favor of the impeachment announced their support. Those columns were added as the last ones in the dataframe.\nI have also assigned names for the respective functions year() and month().\n\nimpeach %>% \n  filter (for_impeachment == \"YES\") %>% \n  mutate(year = year(date_announced), month = month(date_announced))\n\n# A tibble: 209 × 28\n   for_impeachm…¹ last_…² first…³ middl…⁴ party state distr…⁵ date_ann…⁶ margi…⁷\n   <chr>          <chr>   <chr>   <chr>   <chr> <chr> <chr>   <date>     <chr>  \n 1 YES            Adams   Alma    <NA>    D     NC    12      2019-05-31 more_t…\n 2 YES            Aguilar Pete    <NA>    D     CA    31      2019-08-01 more_t…\n 3 YES            Axne    Cynthia <NA>    D     IA    3       2019-09-24 5_poin…\n 4 YES            Barrag… Nanette <NA>    D     CA    44      2019-06-01 more_t…\n 5 YES            Bass    Karen   <NA>    D     CA    37      2019-09-24 more_t…\n 6 YES            Beatty  Joyce   <NA>    D     OH    3       2019-05-31 more_t…\n 7 YES            Bera    Ami     <NA>    D     CA    7       2019-09-24 more_t…\n 8 YES            Beyer   Donald  <NA>    D     VA    8       2019-05-01 more_t…\n 9 YES            Bishop  Sanford D.      D     GA    2       2019-09-24 more_t…\n10 YES            Blumen… Earl    <NA>    D     OR    3       2019-05-03 more_t…\n# … with 199 more rows, 19 more variables: flip_2018 <chr>, house_dist <chr>,\n#   keyrace_rating <chr>, median_income <dbl>,\n#   median_income_compared_to_national <chr>, median_age <dbl>,\n#   median_age_compared_to_national <chr>, pct_nonwhite <dbl>,\n#   pct_nonwhite_compared_to_national <chr>, pct_bachelors <dbl>,\n#   pct_bachelors_compared_to_national <chr>, rural_pop_above20pct <chr>,\n#   gdp_above_national <chr>, clinton_percent <dbl>, trump_percent <dbl>, …\n\n# 2 new columns were created to extract the year and month of the dates announced \n\nAt last, by using the new columns created with the mutate() function, I can again use the group_by function to count how many House of Democrats during each month announced their support of impeachment. To do that, I filtered first to only count people who supported the impeachment (the data already only includes House of Democrats), and, with the mutated columns, I grouped by month and summarised the count for eaach one.\n\nimpeach %>% \n  filter(for_impeachment == \"YES\") %>% \n  mutate(year = year(date_announced), month = month(date_announced)) %>% \n  group_by(month) %>% \n  summarise(n())\n\n# A tibble: 9 × 2\n  month `n()`\n  <dbl> <int>\n1     1     3\n2     4     7\n3     5    39\n4     6    27\n5     7    33\n6     8    18\n7     9    76\n8    11     2\n9    12     4\n\n\n\n\nFunctions\nNow, I am going to walk you through the concepts of functions and how to create them. First, I create a function that will return the sum of 2 numbers by assigning a name to my function. In this case, I assigned ‘sum_two_numbers’ to my function of number x and y. Then, I put ‘return’ to add both values. At last, I add the assign the name ‘result’ to my sum_two_numbers(5,7) and print the result.\n\nsum_two_numbers <- function(x,y){\n  \nreturn(x + y)\n  \n}\n\nresult <- sum_two_numbers (5,7)\nprint(result)\n\n[1] 12\n\n\nNow, I can create a function that will return the mean of a list of numbers fed to it. I use the same step as above but instead of summing values, I am generating their mean. The ‘list’ portion refers to the numbers I assigned in the bottom of the code.\n\nmean_of_numbers <- function(list){\n  return(mean(list))\n}\n\nlist <- c(1,4,6,8,10)\nresult <- mean_of_numbers(list)\nprint(result)\n\n[1] 5.8\n\n\nNow, I expand on the function I made above, and instead of just returning the mean number alone, I have it return a sentence. *“The mean is ___”*. (Where the blank is the mean.)\nI followed the same steps as above by assigning a name to my function and including a list of numbers in the function. After, I just added the text I want generated by using the ‘return’ and the ‘paste0’ commands.\n\nmean_of_numbers_with_text <- function(list){\n  return(paste0(\"The mean is \", mean_of_numbers(list)))\n}\n\nlist <- c(1,4,6,8,10)\nresult <- mean_of_numbers(list)\nprint(result)\n\n[1] 5.8\n\n\nAnother option I did was to create a code that showcases the mean with text. I added a list of numbers and generate its mean with the ‘mean’ command. With the ‘paste0’, I use the text I want printed and the mean I want from the respective list.\n\nnumberlist <- c(12,14,15)\n\nresultingmean <- mean(numberlist)\n\npaste0(\"The mean is \", resultingmean)\n\n[1] \"The mean is 13.6666666666667\"\n\n\nNow, we’ll use the flights dataset from the package nycflights13. It contains records of individual airline flights departing at New York’s three metro airports, JFK, LaGuardia (LGA) and Newark (EWR). Run the following chunk to load the data.\n\ninstall.packages(\"nycflights13\")\n\nInstalling package into '/cloud/lib/x86_64-pc-linux-gnu-library/4.2'\n(as 'lib' is unspecified)\n\n\n\n#load the data to use for the rest of the assignment questions\nflights <- nycflights13::flights\n\nhead(flights)\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n  <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n1  2013     1     1      517         515       2     830     819      11 UA     \n2  2013     1     1      533         529       4     850     830      20 UA     \n3  2013     1     1      542         540       2     923     850      33 AA     \n4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n5  2013     1     1      554         600      -6     812     837     -25 DL     \n6  2013     1     1      554         558      -4     740     728      12 UA     \n# … with 9 more variables: flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>, and abbreviated variable names ¹​sched_dep_time,\n#   ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\nUsing the flights dataset, I created a function that filters the data frame to only include records from a single originating airport (column name origin).\nIn other words, it should let you enter an origin airport and return all the records from just that airport. Then, I create the function I want by assigning a name to it and return the result. In this case I assigned the name ‘filter_flights_by_origin’.\n\n#creating the function\n\nairportname <- \"LGA\"\n\nflights %>% \n  filter(origin == airportname)\n\n# A tibble: 104,662 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      533        529       4     850     830      20 UA     \n 2  2013     1     1      554        600      -6     812     837     -25 DL     \n 3  2013     1     1      557        600      -3     709     723     -14 EV     \n 4  2013     1     1      558        600      -2     753     745       8 AA     \n 5  2013     1     1      559        600      -1     941     910      31 AA     \n 6  2013     1     1      600        600       0     851     858      -7 B6     \n 7  2013     1     1      600        600       0     837     825      12 MQ     \n 8  2013     1     1      602        610      -8     812     820      -8 DL     \n 9  2013     1     1      602        605      -3     821     805      16 MQ     \n10  2013     1     1      623        610      13     920     915       5 AA     \n# … with 104,652 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\nfilter_flights_by_origin <- function(airportname) {\n  result <- flights %>% \n  filter(origin == airportname)\n  \n  return(result)\n}\n\nNow, I use the assigned name to only return the values from the origin I selected.\n\n#returning values \n\nfilter_flights_by_origin(\"LGA\")\n\n# A tibble: 104,662 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      533        529       4     850     830      20 UA     \n 2  2013     1     1      554        600      -6     812     837     -25 DL     \n 3  2013     1     1      557        600      -3     709     723     -14 EV     \n 4  2013     1     1      558        600      -2     753     745       8 AA     \n 5  2013     1     1      559        600      -1     941     910      31 AA     \n 6  2013     1     1      600        600       0     851     858      -7 B6     \n 7  2013     1     1      600        600       0     837     825      12 MQ     \n 8  2013     1     1      602        610      -8     812     820      -8 DL     \n 9  2013     1     1      602        605      -3     821     805      16 MQ     \n10  2013     1     1      623        610      13     920     915       5 AA     \n# … with 104,652 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\nNow, using the flights data set, I created a function that:\nFirst, filters the data frame to only include records from a single originating airport (column name origin)…\n\nairportname <- \"LGA\"\n\nflights %>% \n  filter(origin == airportname) %>% \n  count(carrier)\n\n# A tibble: 13 × 2\n   carrier     n\n   <chr>   <int>\n 1 9E       2541\n 2 AA      15459\n 3 B6       6002\n 4 DL      23067\n 5 EV       8826\n 6 F9        685\n 7 FL       3260\n 8 MQ      16928\n 9 OO         26\n10 UA       8044\n11 US      13136\n12 WN       6087\n13 YV        601\n\n\n… And then ,aggregates the results by airline (carrier) to show how many flights each airline has from that airport. Here, I used the filter, count, and function commands. I combined all of the steps above into one. A different command here is the ‘head’ command that shows the top value of the list since I included the #1 inside the brackets.\n\ncount_byairline_for_one_airport <- function(airportname) {\n  result <- flights %>% \n    filter(origin == airportname) %>% \n    count(carrier, sort = TRUE) %>% #sort = TRUE showcases from the highest value to the lowest \n    head(1) #for the top value of the list \n  \n  return(result)\n}\n\ncount_byairline_for_one_airport(\"EWR\")\n\n# A tibble: 1 × 2\n  carrier     n\n  <chr>   <int>\n1 UA      46087"
  },
  {
    "objectID": "walkthrough.html",
    "href": "walkthrough.html",
    "title": "Analysis Walkthrough Example",
    "section": "",
    "text": "We’ll start by loading. our libraries.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the \u001b]8;;http://conflicted.r-lib.org/\u0007conflicted package\u001b]8;;\u0007 to force all conflicts to become errors\n\n\nCode\nlibrary(lubridate)\n\n\nWe’re going to look at some sample data from Texas on housing prices throughout the state.\nFirst we’ll load data.\n\n\nCode\n# run this line below load the data for this assignment\n# we'll use a built-in dataset from the ggplot2 package (loaded as party of the tidyverse)\nhousesales <- ggplot2::txhousing\n\n\nNow, let’s see what the data looks like\n\n\nCode\nhousesales %>% \n  head()\n\n\n# A tibble: 6 × 9\n  city     year month sales   volume median listings inventory  date\n  <chr>   <int> <int> <dbl>    <dbl>  <dbl>    <dbl>     <dbl> <dbl>\n1 Abilene  2000     1    72  5380000  71400      701       6.3 2000 \n2 Abilene  2000     2    98  6505000  58700      746       6.6 2000.\n3 Abilene  2000     3   130  9285000  58100      784       6.8 2000.\n4 Abilene  2000     4    98  9730000  68600      785       6.9 2000.\n5 Abilene  2000     5   141 10590000  67300      794       6.8 2000.\n6 Abilene  2000     6   156 13910000  66900      780       6.6 2000."
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Paper",
    "section": "",
    "text": "This is a paper I have written when I took the course called Introduction to International Politics (PSC-1003) at the George Washington University with Professor Downes in 2020. The argument revolves around the bargaining model of countries when they go to war and their negotiations methodologies and techniques. Furthermore, the paper explores the trade offs of those methodologies.\n\nThe Bargaining Model\nWar is defined as a “sustained, coordinated violence between political organizations” (Levy & Thompson, 2010, p. 5). As stated in Clausewitz (1996), “the political object is the goal, war is the means of reaching it, and means can never be considered in isolation from their purpose” (p. 87). However, war is not something states normally want. As cited in Fearon (1995), “war can be and often is a product of rational miscalculation” (p. 390), because rational states would normally avoid war due to its high costs. For each state, the value of war is “the share of the good it expects to win, minus the costs it expects to incur” (Frieden, Lake, & Schultz, 2019, p. 98). When states evaluate whether to go to war or to settle, they analyze the costs and values of entering a fight. Assuming that war is costly, states want to avoid it. Therefore, a bargaining range exists. This range is a set of agreements that makes each state at least as well off as it would be by going to war. However, since states live in an anarchic, self-help system there is no credible threat that can enforce states to settle, and without “such credible threat, war will sometimes appear the best option for states that have conflicting interests” (Fearon, 1995, p. 384). The central key for understanding the bargaining model of war is to understand why wars happen if they are so costly. To answer this question the bargaining model assumes five concepts: states are unitary actors that exist in anarchy; states pay the cost of war themselves; states do not prefer war for its own sake; states only care about getting the most valuable deal for themselves; states are rational and weigh costs and benefits. Considering those premises, there are three main reasons for war to occur and for a bargaining range to disappear.\nPrivate information among the international system is one of the key factors that may lead states to war. Given the uncertainty that states face “bargaining strategies that entail a risk of war can be perfectly rational” (Frieden, Lake, & Schultz, 2019, p. 108). This is due to the lack of knowledge that states usually have of their opponent’s military power and resolve to fight. When states have access to others’ private information, the information needs to be analyzed very carefully, for states may have incentives to misrepresent them, leading their opponents’ to overestimate their military power, and underestimate the other’s, creating excessive optimism that may prove to be prejudicial in the future. Therefore, states can be misled by others that share misrepresented information in order to hide their weaknesses. States can indeed “exaggerate their true willingness or capability to fight, if by doing so they might deter future challenges or persuade the other side to make concessions. States can also have an incentive to conceal their capabilities or resolve, if they are concerned that revelation would make them militarily vulnerable or would reduce the chances for a successful first strike” (Fearon, 1995, pp. 395-396). Therefore, leaders can have various reasons to misrepresent private information in order to accomplish their goals, and that, in return, may cause the strike of a war. The article of Lake (2010/11) explained the rationalist explanation for the Iraq war using the bargaining theory. The conflict between the US and Iraq can be seen as due to the misrepresentation of information from Iraq. Saddam Hussein could not state the truth about his WMD capabilities to the US, even though he did not have developed any nuclear weapon, because his enemy, Iran, and other domestic opponents would see him as military weak and vulnerable. Moreover, Saddam believed that the US threat to invade Iraq was a bluff because the US lacked resolve, and would only launch a limited attack against the country, meaning that the US lacked credibility when communicating his threat to Saddam. On the other side, the Bush administration was committed to launching an attack against Iraq and did not see any possibility of settlement with the country, underestimating the costs of post-war governance in the US. The bargaining range between these countries disappeared since the US lacked credibility, Iraq had incentives to misrepresent information, and neither side wanted to know about different information that would have challenged each state’s prior beliefs and slowed the path to war. In the end, Saddam hid the information regarding Iraq’s nuclear weapons, resulting in his removal from power.\nThe second reason why a bargaining range may not exist relies on one of the most accepted theories that explain that the lack of a bargaining range between states is due to “the difficulty that states can have making credible promises not to use force to revise the settlement at a later date” (Frieden, Lake, & Schultz, 2019, p. 118). This is known as the commitment problem. There exists certain goods that can serve as a source of future bargaining power among states. Therefore, as stated in Frieden, Lake, & Schultz (2019), bargaining over such objects (will make the state) be reluctant to render itself more vulnerable to attack without credible promises that the other state will not exploit that vulnerability in the future (p. 118). This causes a preventive war to happen. As stated in Frieden, Lake, & Schultz (2019), this type of war occurs when there is a lack of balance of military capabilities between states because of factors external to the bargaining process, as a power shift in different rates of economic growth and development of new technologies (p. 120). This signifies that there is a rising state and a declining one. As a result, the weaker state may choose to fight now than to fight in the future when it will be worse off. For instance, consider a state A and a state B. When analyzing the possibility of war, “if B’s expected decline in military power is too large relative to B’s costs for war, then state’s A inability to commit to restrain its foreign policy demands after it gains power makes preventive attack rational for state B” (Fearon, 1995, p. 406). A real life example that combines the concept of private information and commitment problem resulting in a preventive war is the War of Triple Alliance. As state through Weisiger (2013) article, Paraguay feared Argentina’s and Brazil’s approximation and conquest of Uruguay, stating that those states would threaten to blockade Paraguay’s only outlet to the sea. Therefore, Paraguay launched a preventive war against Brazil. Lopez, Paraguay’s president, could not commit to any settlement because of his fear of his inevitable decline. In addition, Brazilians believed that Lopez was lying about his reason for attacking the country, so they concluded that Lopez was untrustworthy, unable to commit to any settlement. As a result, Brazil saw the only way to peace as a change in Paraguay’s regime by war. There was no bargaining range from either countries as Paraguay held a strong commitment problem. In addition, private misrepresented information also contributed to the downfall of Lopez. During the war, Paraguay only attacked Argentina due to the private information Lopez received that Argentinians would not follow war against the country in case he attacked Argentina. Therefore, Lopez overestimated his chances of winning and attacked Argentina as well. To his surprise, Lopez’s private information was false, and Argentinians fought against Paraguay, contributing to its defeat.\nAnother concept that favors the idea that it is hard for states to credibly commit to a deal is the offensive advantage, through geography or military technology, that some states may have. These advantages help states strike a preemptive war, for when “first-strike advantages are large, both states must be given more from the peacetime bargain in order to allay the greater temptation of unilateral attack” (Fearon, 1995, p. 403). Therefore, states facing this situation are not concerned on making settlements with each other, but are instead focused on which state will strike first, and “negotiations in this context may be seen as nothing more than a ploy to delay the other side from mobilizing” (Frieden, Lake, & Schultz, 2019, p. 125). For instance, the Six Day War between Israel and the four Arab States (Egypt, Syria, Jordan, and Iraq) was a preemptive war. According to Frieden, Lake, & Schultz (2019), Egypt imposed a partial blockade on Israeli border with Syria in response to Israel’s conflict with the border country (p. 125). Israel felt that Egypt would attack, and saw the opportunity of a first-strike advantage, even though Egypt did not plan to fight. This eliminated any possible bargaining range between states, for Israel now thought that the only way to solve the situation was to strike a first attack. Therefore, Israel launched a surprise attack on Egyptian air force, destroying Egypt’s main offensive weapon, and beginning to control a large territory formerly held by Arabs.\nThe third and last reason why the bargaining range may disappear relates to the indivisibility of a good. Some goods may not be divided among states without losing their value. This is an all or nothing bargaining situation, and “if both states prefer war over getting nothing, then war becomes inevitable” (Frieden, Lake, & Schultz, 2019, p. 127). For instance, Jerusalem is a piece of land that “contains some of the holiest sites of Christianity, Islam, and Judaism, and historical, cultural, and religious significance unlike any other piece of territory in the world” (Frieden, Lake, & Schultz, 2019, p. 128). Therefore, Jerusalem stands as an obstacle to the resolution of conflicts in the Middle East. How to divide the territory and how to ensure that every religion has access to their holy sites is a question that is long for being resolved. That is one of the reasons why countries usually result to war when fighting for this land, for there is no way to ‘divide’ the territory for every religion without losing the territory’s value. Although Jerusalem today is opened for every religion, each would prefer to have the territory for its own. Therefore, Jerusalem is a land that contributes to the religious/territorial conflict in the Middle East.\nThe bargaining model can also be applied to a current conflict in the world: the North Korea vs. US conflict. According to Frieden, Lake, & Schultz (2019), it is unknown what North Korea seeks to accomplish when developing nuclear weapons, and there exists different perspectives on how the US should deal with the country’s approach to nuclear power (pp. 104-105). On the one hand, some people argue that is not possible to negotiate a bargain with North Korea due the state’s commitment problems. The fact that nuclear weapons are a way to reunify Korean peninsula by force reinstates how valuable these weapons are to North Korea and how unlikely negotiations for disarmament will succeed. These people also argue that negotiations in this case serve only to buy North Korea more time to develop its nuclear program, relating to the concept of preemptive war. Another point they make is regarding North Korea’s easiness to cheat to any commitment since the country has an autocratic secret regime, making it easy for it to misrepresent information about its nuclear disarmament. Therefore, North Korea is an untrustworthy country. Contributing to this distrust is North Korea’s history of breaking deals with the US.\nOn the other hand, some people believe that North Korea develops nuclear weapons because of Kim Jong-Un’s fear of being overthrown from power by the US. In that way, negotiations might work, but only if Washington commits not to seek regime change if North Korea lowers its defense, and commits to promote economic assistance and development in the North Korean regime and to reduce US military in Japan and South Korea. President Trump has been advised with different opinions on the matter. Some of his advisors believe that Trump and his allies should put pressure on North Korea to disarm, including having a limited military strike to convince Kim Jong-Un of the costs of defiance by a broader attack designed to eliminate his arsenal. Other advisors, however, believe that this aggressive measure is counterproductive and risky. If North Korea is interested solely in its own survival, then just the threat of retaliation by the US can deter North Korea from using nuclear weapons. In this conflict, the bargaining model is very controversial. Although, there are various set of possible deals between these two countries, one thing is certain, since both countries possess nuclear power, the possibility of war imposes too many costs for it to be gambled. For that reason, Trump is trying to find the most reasonable deal with North Korea to avoid any war from occurring. The reasons for this conflict sometimes to appear not to have any bargaining range is due to North Korea’s commitment problems and misrepresentation of information. However, the consequences of not finding any settlement may result in a nuclear war. Therefore, there needs to exist a bargaining range, for negotiating with North Korea seems as the best diplomatic and rational approach for the resolution of the conflict.\nThe bargaining model does provide a credible explanation for the causes of war. However, it fails to explain certain aspects that may also lead or not to a conflict. First, the model does not consider postwar costs that may influence a state’s decision of going to war or to settle. Second, the theory does not take into account the first image of analysis. Leaders may have motivated and unmotivated biases and specific personalities that may lead states not to have a bargaining range with others. Third, the model does not consider the domestic politics of states such as the regime type, special interests, military influence, and socio-economic problems that may make a country more risk accepting and make it go to war, or more risk-averse and make it prefer any settlement over war. At last, the bargaining model does not take into account multiple states in the brink of war, only two. Therefore, it does not explain how multiple states bargain, and the causes for the occurrence of war among them. As a result, although the bargaining model fails to address other relevant factors when analyzing the causes of war as postwar costs, cognitive biases of decision-makers, states’ domestic politics, and conflicts among multiple states, the model provides a good set of explanations for the origins of conflict between two states.\n```"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Giovanna Lino",
    "section": "",
    "text": "Hi, welcome to my webiste! I’m Giovanna and I’m a student at the George Washington University."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Giovanna Lino",
    "section": "Education",
    "text": "Education\nB.S. Degree in Business with a double concentration in International Business and Finance and a minor in Journalism and Mass Communications from the George Washington University in Washington D.C."
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Giovanna Lino",
    "section": "Skills",
    "text": "Skills\nFinancial analysis and valuation\nReporting and writing\nInbound and Outbound Marketing Strategy"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Giovanna Lino",
    "section": "Interests",
    "text": "Interests\nBusiness podscasts, italian cuisine and fashion."
  },
  {
    "objectID": "newsstory.html",
    "href": "newsstory.html",
    "title": "News Story",
    "section": "",
    "text": "Voter Suppression in Texas\nSeptember, 2022\n   The expression ‘Fraudulent Election’ has risen into prominence once again as former president Donald Trump has questioned the veracity of the results of the 2020 Presidential Election, when he lost to current Democratic President Joe Biden. Texas, a traditional Republican state governed by Greg Abbott, answering to Trump’s claims, has passed a Senate Bill tightening state election laws in 2022, which was retaliated with claims of voter suppression in the state of Texas. \n   In light of Abbott’s third special season in government, Trump has pressured the current Texas Governor to pursue an election audit in most of the state’s counties, claiming voter fraud and irregularities in the 2020 elections. Eight and a half hours later, Abbott’s administration pursued a forensic auditing process on four of the state’s major counties - Collin, Dallas, Harris (Houston included), and Tarrant, showcasing the influence that Trump still has on the state’s Republican party. However, the vote reassessment did not uncover any impropriety, and there is no evidence of fraud in the state during the election period. \n   In response to Abbott’s actions, Trump wrote a letter claiming that Texans “do not trust (Abbott’s) election system, and they want (his) leadership on this issue, which is the number one thing they care about.”\n   In September 2021, Abbott signed a senate bill (SB1), which bans 24 hour voting, drive-thru voting, and imposes difficulties on voting for mail-in-ballot, people with disabilities and marginalized groups. Those measures also protect partisan poll watchers and set limits to those who help other voters, with their commitment of swearing under oath.\n   “What makes this bill and similar ones Republicans are pushing across the country even more un-American is that Republicans are using the ‘Big Lie’ about the 2020 election as a pretext to support them. The reality is that these bills have nothing to do with election integrity or security, but rather are discriminatory measures making it harder for all people to vote. These bills will have a disproportionate impact on communities of color,” stated Eric Holder, the US attorney general under former president Barack Obama. \n   Democratic candidate for Governor, Beto O’Rourke, has argued in Newsweek that “voter suppression is on purpose and by design, working exactly as Greg Abbott intended”. He criticized Abbott over reports that election officials have sent back unapproved mail-in-ballot to voters due to identification problems. Mail-in-ballots affects voters over 65 years of age who cannot provide a correct driver’s license or social security number, military veterans, and overseas voters.\n   Furthermore, The New York Times found that after the primaries held in the beginning of 2022, Harris county, the most populous county in Texas, held that “areas with large Black populations were 44 percent more likely to have ballots rejected than heavily white areas.” Jen Ramos, the political expert for JOLT Action, a progressive club that works with Texan Latinos, told Newsweek that ballot rejections in El Paso, a city with 85% Latinos, was 15 percent during primaries in 2022. The implementation of SB1 has resulted in civil rights groups and democrats filing lawsuits against the new legislation claiming it as an attempt of voter suppression for marginalized communities. \n   “Instead of working on issues that actually matter, like protecting school kids from Covid or fixing our failing electrical grid, Abbott is focused on rigging our elections and implementing extreme, right-wing policies,” said O’Rourke to The New York Times. \n   The Gubernatorial Election of Texas in 2022 will be held with no online voter registration on Election Day; stricter ID policies especially for Latinos, communities of color, and people with disabilities, who are less likely to have an acceptable form of identification on hand; no 24 hour election period accommodation; increased limitations for voter’s assistants; and stricter criminal penalties. \n   Abbott’s administration continues to claim that the bill facilitates instead of suppresses the voting process, but, with all these obstacles, it is worth questioning the real agenda behind the SB1."
  },
  {
    "objectID": "01_virginia_election_project_datawrangling.html",
    "href": "01_virginia_election_project_datawrangling.html",
    "title": "Virginia Election Project",
    "section": "",
    "text": "Data available here: https://historical.elections.virginia.gov/elections/view/144567/\nA little column cleaning and we’ll load in the data file.\n\nprez_2020 <- read_csv(\"processed_data/va_2020_prez_cleaned.csv\")\n\nRows: 134 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): locality\nnum (3): biden, trump, total_votes_2021_prez\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s see what we have\n\nhead(prez_2020) \n\n# A tibble: 6 × 4\n  locality         biden trump total_votes_2021_prez\n  <chr>            <dbl> <dbl>                 <dbl>\n1 Accomack County   7578  9172                 16962\n2 Albemarle County 42466 20804                 64657\n3 Alexandria City  66240 14544                 82508\n4 Alleghany County  2243  5859                  8203\n5 Amelia County     2411  5390                  7893\n6 Amherst County    5672 11041                 17005\n\n\nCalculating percentage of the vote\n\nprez_2020 %>% \n  mutate(\n    biden_pct = biden/total_votes_2021_prez\n  )\n\n# A tibble: 134 × 5\n   locality           biden trump total_votes_2021_prez biden_pct\n   <chr>              <dbl> <dbl>                 <dbl>     <dbl>\n 1 Accomack County     7578  9172                 16962     0.447\n 2 Albemarle County   42466 20804                 64657     0.657\n 3 Alexandria City    66240 14544                 82508     0.803\n 4 Alleghany County    2243  5859                  8203     0.273\n 5 Amelia County       2411  5390                  7893     0.305\n 6 Amherst County      5672 11041                 17005     0.334\n 7 Appomattox County   2418  6702                  9268     0.261\n 8 Arlington County  105344 22318                130699     0.806\n 9 Augusta County     10840 30714                 42278     0.256\n10 Bath County          646  1834                  2501     0.258\n# … with 124 more rows\n\n\nNow let’s do some rounding and move that decimal point\n\nprez_2020 %>% \n  mutate(\n    biden_pct = janitor::round_half_up(biden / total_votes_2021_prez * 100, 1)\n  )\n\n# A tibble: 134 × 5\n   locality           biden trump total_votes_2021_prez biden_pct\n   <chr>              <dbl> <dbl>                 <dbl>     <dbl>\n 1 Accomack County     7578  9172                 16962      44.7\n 2 Albemarle County   42466 20804                 64657      65.7\n 3 Alexandria City    66240 14544                 82508      80.3\n 4 Alleghany County    2243  5859                  8203      27.3\n 5 Amelia County       2411  5390                  7893      30.5\n 6 Amherst County      5672 11041                 17005      33.4\n 7 Appomattox County   2418  6702                  9268      26.1\n 8 Arlington County  105344 22318                130699      80.6\n 9 Augusta County     10840 30714                 42278      25.6\n10 Bath County          646  1834                  2501      25.8\n# … with 124 more rows\n\n\nNow trump too\n\nprez_2020 <- prez_2020 %>% \n  mutate(\n    biden_pct = janitor::round_half_up(biden / total_votes_2021_prez * 100, 2),\n    trump_pct = janitor::round_half_up(trump / total_votes_2021_prez * 100, 2)\n  )\n\nhead(prez_2020)\n\n# A tibble: 6 × 6\n  locality         biden trump total_votes_2021_prez biden_pct trump_pct\n  <chr>            <dbl> <dbl>                 <dbl>     <dbl>     <dbl>\n1 Accomack County   7578  9172                 16962      44.7      54.1\n2 Albemarle County 42466 20804                 64657      65.7      32.2\n3 Alexandria City  66240 14544                 82508      80.3      17.6\n4 Alleghany County  2243  5859                  8203      27.3      71.4\n5 Amelia County     2411  5390                  7893      30.6      68.3\n6 Amherst County    5672 11041                 17005      33.4      64.9"
  },
  {
    "objectID": "01_virginia_election_project_datawrangling.html#reshaping",
    "href": "01_virginia_election_project_datawrangling.html#reshaping",
    "title": "Virginia Election Project",
    "section": "Reshaping",
    "text": "Reshaping\nEnter pivot_wider().\nWe’ll get rid of everything we don’t need first.\n\ngov_2021 <- gov_2021 %>% \n  filter(ballot_name %in% c(\"Glenn A. Youngkin\", \"Terry R. McAuliffe\")) %>% \n  select(-locality_code,\n         -political_party)\n  \ngov_2021\n\n# A tibble: 266 × 4\n   locality_name    ballot_name        votes percentage\n   <chr>            <chr>              <int> <chr>     \n 1 ACCOMACK COUNTY  Glenn A. Youngkin   7878 61.08%    \n 2 ACCOMACK COUNTY  Terry R. McAuliffe  4948 38.37%    \n 3 ALBEMARLE COUNTY Glenn A. Youngkin  19141 37.21%    \n 4 ALBEMARLE COUNTY Terry R. McAuliffe 31919 62.05%    \n 5 ALEXANDRIA CITY  Glenn A. Youngkin  14013 24.02%    \n 6 ALEXANDRIA CITY  Terry R. McAuliffe 43866 75.20%    \n 7 ALLEGHANY COUNTY Glenn A. Youngkin   4530 74.52%    \n 8 ALLEGHANY COUNTY Terry R. McAuliffe  1518 24.97%    \n 9 AMELIA COUNTY    Glenn A. Youngkin   4720 74.19%    \n10 AMELIA COUNTY    Terry R. McAuliffe  1617 25.42%    \n# … with 256 more rows\n\n\nNow we’ll do the spreading out to reshape. One value for each locality\n\ngov_2021_wide <- gov_2021 %>% \n  pivot_wider(names_from = ballot_name, values_from = c(votes, percentage))\n\ngov_2021_wide\n\n# A tibble: 133 × 5\n   locality_name     `votes_Glenn A. Youngkin` votes_Terry R. …¹ perce…² perce…³\n   <chr>                                 <int>             <int> <chr>   <chr>  \n 1 ACCOMACK COUNTY                        7878              4948 61.08%  38.37% \n 2 ALBEMARLE COUNTY                      19141             31919 37.21%  62.05% \n 3 ALEXANDRIA CITY                       14013             43866 24.02%  75.20% \n 4 ALLEGHANY COUNTY                       4530              1518 74.52%  24.97% \n 5 AMELIA COUNTY                          4720              1617 74.19%  25.42% \n 6 AMHERST COUNTY                         9731              3897 71.00%  28.43% \n 7 APPOMATTOX COUNTY                      5971              1438 80.26%  19.33% \n 8 ARLINGTON COUNTY                      21548             73013 22.63%  76.67% \n 9 AUGUSTA COUNTY                        26196              7231 77.93%  21.51% \n10 BATH COUNTY                            1539               396 79.04%  20.34% \n# … with 123 more rows, and abbreviated variable names\n#   ¹​`votes_Terry R. McAuliffe`, ²​`percentage_Glenn A. Youngkin`,\n#   ³​`percentage_Terry R. McAuliffe`\n\n\nNice.\nThis is giving us some pretty long column names. we can change them after the fact using rename(). But first let’s clean the names to make it easier.\n\ngov_2021_wide <- gov_2021_wide %>% \n  clean_names()\n\nhead(gov_2021_wide)\n\n# A tibble: 6 × 5\n  locality_name    votes_glenn_a_youngkin votes_terry_r_mc_aul…¹ perce…² perce…³\n  <chr>                             <int>                  <int> <chr>   <chr>  \n1 ACCOMACK COUNTY                    7878                   4948 61.08%  38.37% \n2 ALBEMARLE COUNTY                  19141                  31919 37.21%  62.05% \n3 ALEXANDRIA CITY                   14013                  43866 24.02%  75.20% \n4 ALLEGHANY COUNTY                   4530                   1518 74.52%  24.97% \n5 AMELIA COUNTY                      4720                   1617 74.19%  25.42% \n6 AMHERST COUNTY                     9731                   3897 71.00%  28.43% \n# … with abbreviated variable names ¹​votes_terry_r_mc_auliffe,\n#   ²​percentage_glenn_a_youngkin, ³​percentage_terry_r_mc_auliffe\n\n\nNow let’s rename, and we’ll use similar names to what we had earlier in our 2021 results.\n\ngov_2021_wide <- gov_2021_wide %>% \n  rename(\n    youngkin = votes_glenn_a_youngkin,\n    mcauliffe = votes_terry_r_mc_auliffe,\n    pct_youngkin = percentage_glenn_a_youngkin,\n    pct_mcauliffe = percentage_terry_r_mc_auliffe\n  )\n\nhead(gov_2021_wide)\n\n# A tibble: 6 × 5\n  locality_name    youngkin mcauliffe pct_youngkin pct_mcauliffe\n  <chr>               <int>     <int> <chr>        <chr>        \n1 ACCOMACK COUNTY      7878      4948 61.08%       38.37%       \n2 ALBEMARLE COUNTY    19141     31919 37.21%       62.05%       \n3 ALEXANDRIA CITY     14013     43866 24.02%       75.20%       \n4 ALLEGHANY COUNTY     4530      1518 74.52%       24.97%       \n5 AMELIA COUNTY        4720      1617 74.19%       25.42%       \n6 AMHERST COUNTY       9731      3897 71.00%       28.43%       \n\n\nBingo.\nThere’s still one potential issue here. Can you see it?\nThe percentage columns are actually text values, not numbers. And they have that % sign in the text too. Let’s fix that using a handy function from the readr package, parse_number().\n\ngov_2021_wide <- gov_2021_wide %>% \n  mutate(\n    pct_youngkin = readr::parse_number(pct_youngkin),\n    pct_mcauliffe = readr::parse_number(pct_mcauliffe)\n  )\n\nhead(gov_2021_wide)\n\n# A tibble: 6 × 5\n  locality_name    youngkin mcauliffe pct_youngkin pct_mcauliffe\n  <chr>               <int>     <int>        <dbl>         <dbl>\n1 ACCOMACK COUNTY      7878      4948         61.1          38.4\n2 ALBEMARLE COUNTY    19141     31919         37.2          62.0\n3 ALEXANDRIA CITY     14013     43866         24.0          75.2\n4 ALLEGHANY COUNTY     4530      1518         74.5          25.0\n5 AMELIA COUNTY        4720      1617         74.2          25.4\n6 AMHERST COUNTY       9731      3897         71            28.4\n\n\nPerfect. Problem solved."
  },
  {
    "objectID": "01_virginia_election_project_datawrangling.html#comparing-gov-vs.-prez-results",
    "href": "01_virginia_election_project_datawrangling.html#comparing-gov-vs.-prez-results",
    "title": "Virginia Election Project",
    "section": "Comparing gov vs. prez results",
    "text": "Comparing gov vs. prez results\nNow that things are join, let’s actually go ahead and start making columns to compare the two elections and how the candidates did this time compared with last time.\nWhere should we go from here….? Give it a shot…\n\njoined_vacomparison %>% \n  mutate(\n    mc_overperform = pct_mcauliffe - biden_pct,\n    mc_overperform_worsethan5 = if_else(mc_overperform < -5, \"YES\", \"NO\") #created a flag column \n    \n  )\n\n# A tibble: 133 × 11\n   locality  biden trump biden…¹ trump…² young…³ mcaul…⁴ pct_y…⁵ pct_m…⁶ mc_ov…⁷\n   <chr>     <dbl> <dbl>   <dbl>   <dbl>   <int>   <int>   <dbl>   <dbl>   <dbl>\n 1 ACCOMAC…   7578  9172    44.7    54.1    7878    4948    61.1    38.4   -6.31\n 2 ALBEMAR…  42466 20804    65.7    32.2   19141   31919    37.2    62.0   -3.63\n 3 ALEXAND…  66240 14544    80.3    17.6   14013   43866    24.0    75.2   -5.08\n 4 ALLEGHA…   2243  5859    27.3    71.4    4530    1518    74.5    25.0   -2.37\n 5 AMELIA …   2411  5390    30.6    68.3    4720    1617    74.2    25.4   -5.13\n 6 AMHERST…   5672 11041    33.4    64.9    9731    3897    71      28.4   -4.92\n 7 APPOMAT…   2418  6702    26.1    72.3    5971    1438    80.3    19.3   -6.76\n 8 ARLINGT… 105344 22318    80.6    17.1   21548   73013    22.6    76.7   -3.93\n 9 AUGUSTA…  10840 30714    25.6    72.6   26196    7231    77.9    21.5   -4.13\n10 BATH CO…    646  1834    25.8    73.3    1539     396    79.0    20.3   -5.49\n# … with 123 more rows, 1 more variable: mc_overperform_worsethan5 <chr>, and\n#   abbreviated variable names ¹​biden_pct, ²​trump_pct, ³​youngkin, ⁴​mcauliffe,\n#   ⁵​pct_youngkin, ⁶​pct_mcauliffe, ⁷​mc_overperform\n\n\n\n#save results to file for next step\nsaveRDS(joined_vacomparison, here(\"processed_data\", \"joined_vacomparison.rds\"))\nwrite_csv(joined_vacomparison, here(\"processed_data\", \"joined_vacomparison.csv\"))"
  },
  {
    "objectID": "censusmap.html",
    "href": "censusmap.html",
    "title": "Census Map",
    "section": "",
    "text": "In this walkthrough, I will demonstrate how to create an interactive county census map of a chosen state using R. Specifically, I will be using the tidycensus package to download census data from the American Community Survey and the tmap and mapview packages to create the interactive map.\nI have chosen the state of Texas and will be mapping the percentage of individuals over the age of 25 with a bachelor’s degree or higher."
  },
  {
    "objectID": "censusmap.html#step-1-install-packages",
    "href": "censusmap.html#step-1-install-packages",
    "title": "Census Map",
    "section": "Step 1: Install Packages",
    "text": "Step 1: Install Packages\nFirst, make sure you have the necessary packages installed and loaded. In this case I have used the following packages:\n\ninstall.packages(\"tidycensus\")\n\nInstalling package into '/cloud/lib/x86_64-pc-linux-gnu-library/4.2'\n(as 'lib' is unspecified)\n\ninstall.packages(\"tmap\")\n\nInstalling package into '/cloud/lib/x86_64-pc-linux-gnu-library/4.2'\n(as 'lib' is unspecified)\n\ninstall.packages(\"mapview\")\n\nInstalling package into '/cloud/lib/x86_64-pc-linux-gnu-library/4.2'\n(as 'lib' is unspecified)\n\n# Load packages\nlibrary(tidycensus)\nlibrary(tmap)\nlibrary(mapview)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "censusmap.html#step-2-set-up-census-api-key",
    "href": "censusmap.html#step-2-set-up-census-api-key",
    "title": "Census Map",
    "section": "Step 2: Set up Census API Key",
    "text": "Step 2: Set up Census API Key\nTo download data from the American Community Survey using the tidycensus package, you need to set up a Census API key.\nWith the API key,set it up in R using the following code:\n\n# Census API key\n#Retrived from the API Link on their page \n\ncensus_api_key(\"75fa32dac9122cf2c43fb6e5d0ed1e7ead626185\")\n\nTo install your API key for use in future sessions, run this function with `install = TRUE`."
  },
  {
    "objectID": "censusmap.html#step-3-download-census-data",
    "href": "censusmap.html#step-3-download-census-data",
    "title": "Census Map",
    "section": "Step 3: Download Census Data",
    "text": "Step 3: Download Census Data\nDownload the census data for Texas using the tidycensus package. Specifically, we will be downloading the percentage of individuals over the age of 25 with a bachelor’s degree or higher.\n\n# Download census data\ntx_census <- get_acs(\n  geography = \"county\",\n  variables = \"B15003_022\",\n  state = \"TX\",\n  survey = \"acs5\",\n  year = 2019,\n  geometry = TRUE)\n\nGetting data from the 2015-2019 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%"
  },
  {
    "objectID": "censusmap.html#step-4-join-census-data-with-spatial-data",
    "href": "censusmap.html#step-4-join-census-data-with-spatial-data",
    "title": "Census Map",
    "section": "Step 4: Join Census Data with Spatial Data",
    "text": "Step 4: Join Census Data with Spatial Data\nNow, join the census data with spatial data for Texas counties. We will be using the tigris package to get the county shapefiles and join them with the census data using the left_join function.\n\n# Load tigris package\nlibrary(tigris)\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\n# Texas county shapefiles\ntx_counties <- counties(state = \"TX\", cb = TRUE)\n\nRetrieving data for the year 2021\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%"
  },
  {
    "objectID": "censusmap.html#step-5-create-choropleth-map",
    "href": "censusmap.html#step-5-create-choropleth-map",
    "title": "Census Map",
    "section": "Step 5: Create Choropleth Map",
    "text": "Step 5: Create Choropleth Map\nCreate a choropleth map of our data using the tmap package.\n\ntm_shape(tx_census) +\n  tm_polygons(\"estimate\",\n              border.col = \"black\",\n              style = \"quantile\",\n              palette = \"Blues\",\n              title = \"Percentage of Individuals with more than 25 with Bachelor's Degree or Higher\") +\n  tm_layout(main.title = \"Texas Counties\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Welcome to my portfolio. Additional past works are included in this page, which takes you to my Linkedln. profile."
  },
  {
    "objectID": "portfolio.html#burberry-the-brands-dilemma-with-sustainability",
    "href": "portfolio.html#burberry-the-brands-dilemma-with-sustainability",
    "title": "Portfolio",
    "section": "Burberry: the brand’s dilemma with sustainability",
    "text": "Burberry: the brand’s dilemma with sustainability\nDecember, 2020\n   The fashion industry is the second largest polluter in the world and consumers are becoming more aware of this. There has been a push to prioritize companies that acknowledge the value of sustainability, and hesitation to support those that fail to address it. Sustainability is a primary concern for consumers and in order for companies to continue to optimize their sales and remain relevant, their designers and managers must address the topic immediately.\n   When it comes to sustainability, companies have different opinions. On one hand, some argue that becoming sustainable is too costly and not worth the effort. While on the other hand, some brands view this extra cost as something that will be compensated by new clientele. However, it seems that the vast majority of Millenial, Gen X, Gen Z consumers believe that sustainability is a significant issue and that it should be addressed in the fashion industry. \n   With the influx of these concerned consumers, brands must adjust their strategies and undertake the topic of sustainability to stay competitive. For example, in the luxury fashion industry, fur has always been an important medium. In recent years, there has been a decrease in demand for the luxurious animal by product because there is a heightened awareness of the detrimental effects of the fur industry. For that reason, some designers are beginning to swap fur for something faux. In my opinion, this shift is beneficial and will have a positive impact on a luxury brand’s reputation.\n   One high end brand that is highly invested in sustainability is Burberry, the British fashion house created in 1865 by Thomas Burberry. The company proved to be unique in the high fashion market when Thomas created Gabardine, a ‘breathable, weatherproof and hardwearing fabric revolutionising rainwear’. Also adding to the originality of the brand is Thomas Burberry’s creation of the iconic pattern-based scarves, purses, and trench coats. In 2017, when the company’s annual report was released, environmental activists protested the brand because it had burned almost $29 million dollars of unsold inventory in a year. With the major repercussion of the brand’s lack of sustainability commitment, social media backlash, the movement #Burnberry arose to demand change. The protests proved to be effective since Burberry is now dedicated to improving sustainability inside the company.\n   As of now, the brand uses 58% of its energy and 68% of its electricity from clean energy sources and Burberry plans to stay committed to sustainability in the future. The brand has made several promises to become more sustainable for 2022. For instance, Burberry has committed to stopping the use of real fur and burning unwanted items, become carbon neutral by 2022, and operate entirely by renewable energy. In addition, Burberry has released a ‘Responsibility Agenda’ to set goals for increasing sustainability within the company. The agenda focuses on three areas: product, company, and communities. Each area is centered on achieving progress by making the brand’s product more recyclable, ridding the practice of burning unsaleable products, and impacting teachers and students by increasing mentoring programs in developing regions.\n   The ‘Responsibility Agenda’ goals abides with the Paris Climate Agreement and the UN’s 17 Sustainable Development Goals. Burberry committed to becoming a 100% renewable energy run company, and to create a Manufacturing Excellence Program focused on enhancing worker wellbeing in the company’s supply chain. The brand has been recognized with two awards for its sustainable fashion approach: Leading luxury brand in the ‘Textiles, Apparel & Luxury Goods’ sector by the DJSI 2018, and ‘Bronze Class’ in RobecoSAM’s 2018 Sustainability Yearbook.\n   Furthermore, Burberry established partnerships aimed at increasing sustainability. First, Burberry collaborated with Elvis & Kresse, a sustainable company devoted to rescuing raw materials and transforming it into luxury accessories, to make 120 tons of wasted leather fabric into selling products. Second, the brand has launched a capsule collection made from Econyl, sustainable nylon made from regenerated fishing nets, fabric scraps and industrial plastic, developing a way to combat plastic waste while creating a circular fashion filled with luxury products.\n   Burberry is one example of a company that suffered political turmoil due to its unsustainable approaches to fashion. Now, the brand is fixing the problem by collaborating with sustainable brands, becoming more transparent with its consumers and stakeholders, and setting goals to increase sustainability at the company. My hope is that Burberry becomes an example for other haute couture to follow."
  },
  {
    "objectID": "portfolio.html#psychedelic-patterns",
    "href": "portfolio.html#psychedelic-patterns",
    "title": "Portfolio",
    "section": "Psychedelic Patterns",
    "text": "Psychedelic Patterns\nOctober, 2021\n\n   Fashion is always trying to reinvent itself. During the pandemic, various fashion trends were revived. One in particular was psychedelic patterns. Inspired by the 70s, this type of print was first popularized by counterculture teenagers in the midst of social and political turmoil. The ‘LSD fabric’ made of colorful lines refers to a ‘hallucinogenic’’ pattern that became the inspiration for other psychedelic shapes, and made it possible for the pattern to be produced quickly and appear as a ‘groovy’ style in movies. These patterns are known for their bold colors, sinuous lines and natural shapes.  \n   In 2021, with the use of social media, this style is back in action. Tie dye is one of the most popular psychedelic patterns that has been revived. According to the trend-forecasting company Heuritech, during this Winter season, tie dye trends are expected to increase more than 6% when compared to previous seasons.  High end brands, along with local boutiques, have embraced this sense of nostalgia and have created collections based on this style. Brands such as Tom Ford, Collina Strada, Lecavalier, and Raf Simons have embraced the tie dye and other psychedelic prints in their Spring/Summer 2021 collections. \n   Raf Simons used colorblocked and defined swirls. The collection was named ‘Teenage Dreams’ inspired by the 1979 film Hair, which resembles the feeling of uncertainty and change that people are living nowadays. The pandemic has had a major impact on the return of the psychedelic. Dennis Nothdruft, head of exhibitions in the Fashion & Textile Museum in London, said that there is a feeling  of civil unrest and change that correlates with the use of different prints. There is nothing more ‘youth revolt’ than what occurred in the 70s and the current moment we are facing. With the return of these prints, there is an evocation for community and participation, providing sensory experiences to the fashion community. \n   One fashion commentator said that during the pandemic, people got used to looking at social media all the time. For a brand to stand out, it requires a creative collection, and nifty patterns are a way to achieve that. Ideally, a user would stop scrolling and wonder more about the brand when coming across a stylish psychedelic pattern dress. Psychedelic patterns are here to bring back that sense of revolt and freedom that people need in 2021, a freedom that is colorful and bold."
  },
  {
    "objectID": "portfolio.html#fashion-reinvention---rihanna-vs.-savage-x-fenty",
    "href": "portfolio.html#fashion-reinvention---rihanna-vs.-savage-x-fenty",
    "title": "Portfolio",
    "section": "Fashion reinvention - Rihanna vs. Savage X Fenty",
    "text": "Fashion reinvention - Rihanna vs. Savage X Fenty\nOctober, 2020\n   On September 30, 2020, Rihanna showed her newest Savage X Fenty collection to all fashion lovers. Known for having an open and liberal approach to the fashion industry, Rihanna delivered a fashion show filled with diversity and inclusion, with models who identified as men, women, non-binary, and celebrities models as Paris Hilton, Lizzo, Bella Hadid, Normani, Cara Delavigne, Shea Couleé, among many others.  \n   Rihanna started to add to the diversity conversation in fashion when she launched Fenty in 2017, being the first woman of color to be added to the LVMH group. From then, she started to portray diversity and body acceptance in all of her collections and fashion shows, and the 2020 fashion show was no different.\n   During the pandemic, people started to question the meaning of traditional fashion, and how can the industry be more inclusive. Quarantine gave people a lot of time to think about this aspect, giving more freedom to reinvent the industry and history of fashion shows. Rihanna was one of the designers who invested in this topic and brought to the public a fashion show that empowered people’s sexuality and encouraged all to take ownership and pride of their bodies when using lingeries. \n   By taking this inclusive approach, Rihanna impacts not only the fashion industry, but also millions of followers, fans, and consumers who can now envision themselves in all Savage X Fenty lingeries. They feel represented, and that is key for both sales success, and, most importantly, for the reinvention of the fashion industry. Furthermore, it is necessary to understand that the show is not simply a marketing strategy. Rihanna once stated, “I want to make stuff that I can see on the people that I know and they come in all different shapes, sizes, races, religions.” It is a part of who she is, and her brand showcases that. Therefore, it is more than just an opportunity taken to increase sales by jumping on a trending moment of body acceptance. It actually is the core value of the brand and of the designer. \n   Another concept incorporated in this show was a virtual theatrical experience. If you watched it, you know that the show was filled with dancers and performances. This unique model enhanced the powerfulness of the show when portraying diversity of genders, races, body heights, and weights. It was a very creative approach in comparison to regular fashion shows, especially when it is important to keep consumers’ attention to the show now that it has a virtual format. The performance is available on Amazon prime. It is shocking how a year ago this was never considered a possibility, and now, it’s the new norm. \n   In a world where Victoria Secrets’ scandal showed that there is still low tolerance when it comes to diversifying lingerie models in the runways, Rihanna questions this status quo with the release of her 2020 Savage X Fenty collection, and does that by capturing its public with an incredible virtual experience!"
  },
  {
    "objectID": "virginiaelectionresults.html",
    "href": "virginiaelectionresults.html",
    "title": "Virginia Election Project",
    "section": "",
    "text": "Data available here: https://historical.elections.virginia.gov/elections/view/144567/\nA little column cleaning and we’ll load in the data file.\n\nprez_2020 <- read_csv(\"processed_data/va_2020_prez_cleaned.csv\")\n\nRows: 134 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): locality\nnum (3): biden, trump, total_votes_2021_prez\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s see what we have\n\nhead(prez_2020) \n\n# A tibble: 6 × 4\n  locality         biden trump total_votes_2021_prez\n  <chr>            <dbl> <dbl>                 <dbl>\n1 Accomack County   7578  9172                 16962\n2 Albemarle County 42466 20804                 64657\n3 Alexandria City  66240 14544                 82508\n4 Alleghany County  2243  5859                  8203\n5 Amelia County     2411  5390                  7893\n6 Amherst County    5672 11041                 17005\n\n\nCalculating percentage of the vote\n\nprez_2020 %>% \n  mutate(\n    biden_pct = biden/total_votes_2021_prez\n  )\n\n# A tibble: 134 × 5\n   locality           biden trump total_votes_2021_prez biden_pct\n   <chr>              <dbl> <dbl>                 <dbl>     <dbl>\n 1 Accomack County     7578  9172                 16962     0.447\n 2 Albemarle County   42466 20804                 64657     0.657\n 3 Alexandria City    66240 14544                 82508     0.803\n 4 Alleghany County    2243  5859                  8203     0.273\n 5 Amelia County       2411  5390                  7893     0.305\n 6 Amherst County      5672 11041                 17005     0.334\n 7 Appomattox County   2418  6702                  9268     0.261\n 8 Arlington County  105344 22318                130699     0.806\n 9 Augusta County     10840 30714                 42278     0.256\n10 Bath County          646  1834                  2501     0.258\n# … with 124 more rows\n\n\nNow let’s do some rounding and move that decimal point\n\nprez_2020 %>% \n  mutate(\n    biden_pct = janitor::round_half_up(biden / total_votes_2021_prez * 100, 1)\n  )\n\n# A tibble: 134 × 5\n   locality           biden trump total_votes_2021_prez biden_pct\n   <chr>              <dbl> <dbl>                 <dbl>     <dbl>\n 1 Accomack County     7578  9172                 16962      44.7\n 2 Albemarle County   42466 20804                 64657      65.7\n 3 Alexandria City    66240 14544                 82508      80.3\n 4 Alleghany County    2243  5859                  8203      27.3\n 5 Amelia County       2411  5390                  7893      30.5\n 6 Amherst County      5672 11041                 17005      33.4\n 7 Appomattox County   2418  6702                  9268      26.1\n 8 Arlington County  105344 22318                130699      80.6\n 9 Augusta County     10840 30714                 42278      25.6\n10 Bath County          646  1834                  2501      25.8\n# … with 124 more rows\n\n\nNow trump too\n\nprez_2020 <- prez_2020 %>% \n  mutate(\n    biden_pct = janitor::round_half_up(biden / total_votes_2021_prez * 100, 2),\n    trump_pct = janitor::round_half_up(trump / total_votes_2021_prez * 100, 2)\n  )\n\nhead(prez_2020)\n\n# A tibble: 6 × 6\n  locality         biden trump total_votes_2021_prez biden_pct trump_pct\n  <chr>            <dbl> <dbl>                 <dbl>     <dbl>     <dbl>\n1 Accomack County   7578  9172                 16962      44.7      54.1\n2 Albemarle County 42466 20804                 64657      65.7      32.2\n3 Alexandria City  66240 14544                 82508      80.3      17.6\n4 Alleghany County  2243  5859                  8203      27.3      71.4\n5 Amelia County     2411  5390                  7893      30.6      68.3\n6 Amherst County    5672 11041                 17005      33.4      64.9"
  },
  {
    "objectID": "virginiaelectionresults.html#reshaping",
    "href": "virginiaelectionresults.html#reshaping",
    "title": "Virginia Election Project",
    "section": "Reshaping",
    "text": "Reshaping\nEnter pivot_wider().\nWe’ll get rid of everything we don’t need first.\n\ngov_2021 <- gov_2021 %>% \n  filter(ballot_name %in% c(\"Glenn A. Youngkin\", \"Terry R. McAuliffe\")) %>% \n  select(-locality_code,\n         -political_party)\n  \ngov_2021\n\n# A tibble: 266 × 4\n   locality_name    ballot_name        votes percentage\n   <chr>            <chr>              <int> <chr>     \n 1 ACCOMACK COUNTY  Glenn A. Youngkin   7878 61.08%    \n 2 ACCOMACK COUNTY  Terry R. McAuliffe  4948 38.37%    \n 3 ALBEMARLE COUNTY Glenn A. Youngkin  19141 37.21%    \n 4 ALBEMARLE COUNTY Terry R. McAuliffe 31919 62.05%    \n 5 ALEXANDRIA CITY  Glenn A. Youngkin  14013 24.02%    \n 6 ALEXANDRIA CITY  Terry R. McAuliffe 43866 75.20%    \n 7 ALLEGHANY COUNTY Glenn A. Youngkin   4530 74.52%    \n 8 ALLEGHANY COUNTY Terry R. McAuliffe  1518 24.97%    \n 9 AMELIA COUNTY    Glenn A. Youngkin   4720 74.19%    \n10 AMELIA COUNTY    Terry R. McAuliffe  1617 25.42%    \n# … with 256 more rows\n\n\nNow we’ll do the spreading out to reshape. One value for each locality\n\ngov_2021_wide <- gov_2021 %>% \n  pivot_wider(names_from = ballot_name, values_from = c(votes, percentage))\n\ngov_2021_wide\n\n# A tibble: 133 × 5\n   locality_name     `votes_Glenn A. Youngkin` votes_Terry R. …¹ perce…² perce…³\n   <chr>                                 <int>             <int> <chr>   <chr>  \n 1 ACCOMACK COUNTY                        7878              4948 61.08%  38.37% \n 2 ALBEMARLE COUNTY                      19141             31919 37.21%  62.05% \n 3 ALEXANDRIA CITY                       14013             43866 24.02%  75.20% \n 4 ALLEGHANY COUNTY                       4530              1518 74.52%  24.97% \n 5 AMELIA COUNTY                          4720              1617 74.19%  25.42% \n 6 AMHERST COUNTY                         9731              3897 71.00%  28.43% \n 7 APPOMATTOX COUNTY                      5971              1438 80.26%  19.33% \n 8 ARLINGTON COUNTY                      21548             73013 22.63%  76.67% \n 9 AUGUSTA COUNTY                        26196              7231 77.93%  21.51% \n10 BATH COUNTY                            1539               396 79.04%  20.34% \n# … with 123 more rows, and abbreviated variable names\n#   ¹​`votes_Terry R. McAuliffe`, ²​`percentage_Glenn A. Youngkin`,\n#   ³​`percentage_Terry R. McAuliffe`\n\n\nNice.\nThis is giving us some pretty long column names. we can change them after the fact using rename(). But first let’s clean the names to make it easier.\n\ngov_2021_wide <- gov_2021_wide %>% \n  clean_names()\n\nhead(gov_2021_wide)\n\n# A tibble: 6 × 5\n  locality_name    votes_glenn_a_youngkin votes_terry_r_mc_aul…¹ perce…² perce…³\n  <chr>                             <int>                  <int> <chr>   <chr>  \n1 ACCOMACK COUNTY                    7878                   4948 61.08%  38.37% \n2 ALBEMARLE COUNTY                  19141                  31919 37.21%  62.05% \n3 ALEXANDRIA CITY                   14013                  43866 24.02%  75.20% \n4 ALLEGHANY COUNTY                   4530                   1518 74.52%  24.97% \n5 AMELIA COUNTY                      4720                   1617 74.19%  25.42% \n6 AMHERST COUNTY                     9731                   3897 71.00%  28.43% \n# … with abbreviated variable names ¹​votes_terry_r_mc_auliffe,\n#   ²​percentage_glenn_a_youngkin, ³​percentage_terry_r_mc_auliffe\n\n\nNow let’s rename, and we’ll use similar names to what we had earlier in our 2021 results.\n\ngov_2021_wide <- gov_2021_wide %>% \n  rename(\n    youngkin = votes_glenn_a_youngkin,\n    mcauliffe = votes_terry_r_mc_auliffe,\n    pct_youngkin = percentage_glenn_a_youngkin,\n    pct_mcauliffe = percentage_terry_r_mc_auliffe\n  )\n\nhead(gov_2021_wide)\n\n# A tibble: 6 × 5\n  locality_name    youngkin mcauliffe pct_youngkin pct_mcauliffe\n  <chr>               <int>     <int> <chr>        <chr>        \n1 ACCOMACK COUNTY      7878      4948 61.08%       38.37%       \n2 ALBEMARLE COUNTY    19141     31919 37.21%       62.05%       \n3 ALEXANDRIA CITY     14013     43866 24.02%       75.20%       \n4 ALLEGHANY COUNTY     4530      1518 74.52%       24.97%       \n5 AMELIA COUNTY        4720      1617 74.19%       25.42%       \n6 AMHERST COUNTY       9731      3897 71.00%       28.43%       \n\n\nBingo.\nThere’s still one potential issue here. Can you see it?\nThe percentage columns are actually text values, not numbers. And they have that % sign in the text too. Let’s fix that using a handy function from the readr package, parse_number().\n\ngov_2021_wide <- gov_2021_wide %>% \n  mutate(\n    pct_youngkin = readr::parse_number(pct_youngkin),\n    pct_mcauliffe = readr::parse_number(pct_mcauliffe)\n  )\n\nhead(gov_2021_wide)\n\n# A tibble: 6 × 5\n  locality_name    youngkin mcauliffe pct_youngkin pct_mcauliffe\n  <chr>               <int>     <int>        <dbl>         <dbl>\n1 ACCOMACK COUNTY      7878      4948         61.1          38.4\n2 ALBEMARLE COUNTY    19141     31919         37.2          62.0\n3 ALEXANDRIA CITY     14013     43866         24.0          75.2\n4 ALLEGHANY COUNTY     4530      1518         74.5          25.0\n5 AMELIA COUNTY        4720      1617         74.2          25.4\n6 AMHERST COUNTY       9731      3897         71            28.4\n\n\nPerfect. Problem solved."
  },
  {
    "objectID": "virginiaelectionresults.html#comparing-gov-vs.-president-results",
    "href": "virginiaelectionresults.html#comparing-gov-vs.-president-results",
    "title": "Virginia Election Project",
    "section": "Comparing gov vs. president results",
    "text": "Comparing gov vs. president results\nNow that things are join, let’s actually go ahead and start making columns to compare the two elections and how the candidates did this time compared with last time.\nWhere should we go from here….? Give it a shot…\n\njoined_vacomparison %>% \n  mutate(\n    mc_overperform = pct_mcauliffe - biden_pct,\n    mc_overperform_worsethan5 = if_else(mc_overperform < -5, \"YES\", \"NO\")\n    #created a flag column \n    \n  )\n\n# A tibble: 133 × 11\n   locality  biden trump biden…¹ trump…² young…³ mcaul…⁴ pct_y…⁵ pct_m…⁶ mc_ov…⁷\n   <chr>     <dbl> <dbl>   <dbl>   <dbl>   <int>   <int>   <dbl>   <dbl>   <dbl>\n 1 ACCOMAC…   7578  9172    44.7    54.1    7878    4948    61.1    38.4   -6.31\n 2 ALBEMAR…  42466 20804    65.7    32.2   19141   31919    37.2    62.0   -3.63\n 3 ALEXAND…  66240 14544    80.3    17.6   14013   43866    24.0    75.2   -5.08\n 4 ALLEGHA…   2243  5859    27.3    71.4    4530    1518    74.5    25.0   -2.37\n 5 AMELIA …   2411  5390    30.6    68.3    4720    1617    74.2    25.4   -5.13\n 6 AMHERST…   5672 11041    33.4    64.9    9731    3897    71      28.4   -4.92\n 7 APPOMAT…   2418  6702    26.1    72.3    5971    1438    80.3    19.3   -6.76\n 8 ARLINGT… 105344 22318    80.6    17.1   21548   73013    22.6    76.7   -3.93\n 9 AUGUSTA…  10840 30714    25.6    72.6   26196    7231    77.9    21.5   -4.13\n10 BATH CO…    646  1834    25.8    73.3    1539     396    79.0    20.3   -5.49\n# … with 123 more rows, 1 more variable: mc_overperform_worsethan5 <chr>, and\n#   abbreviated variable names ¹​biden_pct, ²​trump_pct, ³​youngkin, ⁴​mcauliffe,\n#   ⁵​pct_youngkin, ⁶​pct_mcauliffe, ⁷​mc_overperform\n\n\n\n#save results to file for next step\nsaveRDS(joined_vacomparison, here(\"processed_data\", \"joined_vacomparison.rds\"))\nwrite_csv(joined_vacomparison, here(\"processed_data\", \"joined_vacomparison.csv\"))"
  },
  {
    "objectID": "breakingnewsstory.html",
    "href": "breakingnewsstory.html",
    "title": "Breaking News Story",
    "section": "",
    "text": "51.4% of Trump’s voters are part of the counties above national average for people who identify as nonwhite.\nIn the aftermath of the 2020 Presidential campaign, both the results of the election and the demographics of the voter pool of each county were analyzed for both the Republican candidate Donald Trump and the Democratic candidate Joe Biden. Particularly, Trump’s mean percentage of votes by U.S. counties was analyzed for both the number of nonwhite voters and the number of college graduates and if they were above or below the U.S. national average.\nIt comes as a surprise that more than 51.4 percent of Trump’s voters are part of the counties above the national average for people who identify as nonwhite, while approximately 68.5 of votes are part of counties below the national average for non white-identified people. Furthermore, on average, 45 percent of Trump’s votes come from counties with people above the national average for college graduates, versus 68.2 percent below the national average for college graduates.\nThese results come as a surprise since it is known that the Democratic party has the majority of college graduate and nonwhite votes. Although this is still the case, the Democratic party cannot solely rely on these metrics to ensure victory since more college graduates and nonwhite voters can be switching their minds and increasing their support for the Republican party.\nThis trend can be seen in states such as Texas. Some working-class Hispanics did not feel completely represented by the Democratic party in the last gubernatorial race in November, 2022. The Democratic candidate, Beto O’Rourke is a citizen of El Paso and has been called out by activists from the organization Sunrise El Paso for not supporting them in their fight against the privatization of their electric companies. The city of El Paso is not on the Texas electrical grid, allowing for the company J.P Morgan to privatize it and charge expensive bills. Representatives of Sunrise El Paso claim that O’Rourke is too much of a Central Democrat and does not want to compromise the funding of his campaign by selecting fights with big corporations. The midterm results showcased that O’Rourke lost to Republican candidate Greg Abbott, and that Abbott secured a good portion of votes from the Hispanic community."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! My name is Giovanna Romariz Lino. I am 22 years old and I am originally from Londrina, Brazil. I have just graduated from George Washington University with a Bachelor of Science degree in Business with a double concentration in International Business and Finance and a minor in Journalism and Mass Communications. During university, I have held leadership roles in the Brazilian Student Association, Alpha Kappa Psi Business Fraternity, the GW Socially Responsible Fund, among others. I have spent 4 years living in Washington D.C., and I have also studied abroad in Milan, Italy, at Bocconi University during Spring, 2022. During my life, I have also lived in Canada, and done summer exchanges in the United States while I was in high school. I speak fluent English, Portuguese, intermediate Spanish, and basic Italian. I am an only child and my parents still live in Brazil.\nDuring my time at university, I have interned in more than four companies in Brazil focused on finance and marketing. I have also participated in consulting projects in Washington D.C. My main focuses of work following graduation are on consulting and finance. Now, I am moving to New York in July, 2023, and joining Alphasights as an Associate working under their Consulting department.\nIf you want to know more about my professional experience, you can read my resume."
  }
]